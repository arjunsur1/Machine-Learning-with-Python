{"cells":[{"cell_type":"markdown","source":["##Importing Libraries"],"metadata":{}},{"cell_type":"code","source":["import os\nimport pandas as pd\nimport numpy as np\n\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\n\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as fn\nfrom pyspark.sql.functions import col, udf\n\nfrom pyspark.ml.stat import Correlation\n\nfrom pyspark.ml.classification import LogisticRegression\n\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n\nfrom pyspark.ml.feature import Bucketizer, StringIndexer, OneHotEncoder, StandardScaler, VectorAssembler\n\nfrom pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.mllib.evaluation import MulticlassMetrics\n\nfrom pyspark.ml import regression\nfrom pyspark.ml import feature\nfrom pyspark.ml import Pipeline"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["import seaborn as sns\nimport matplotlib.pyplot as plt"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Starting a new Spark Session"],"metadata":{}},{"cell_type":"code","source":["spark = (SparkSession\n         .builder\n         .master(\"local[*]\")\n         .appName(\"predict mortgage approval\")\n         .getOrCreate())\nspark"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["sc = spark.sparkContext\nsc"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["sqlContext = SQLContext(spark.sparkContext)\nsqlContext"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Load training data and cache it. We will be using this data set over and over again.\nmortgage = (spark.read.csv(path='/FileStore/tables/ny_hmda_2015.csv',header=True,inferSchema=True, ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True).cache())"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["display(mortgage)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["mortgage.printSchema()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["mortgage=mortgage.toPandas()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["mortgage.info()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["mortgage[['action_taken_name','action_taken']]"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["mortgage['purchaser_type_name']"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["mortgage.columns"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["cols=['action_taken_name','agency_abbr','agency_name','applicant_ethnicity_name','applicant_race_2','applicant_race_3','applicant_race_4','applicant_race_5',\\\n     'applicant_race_name_1', 'applicant_race_name_2','applicant_race_name_3', 'applicant_race_name_4','applicant_race_name_5','applicant_sex_name','co_applicant_ethnicity_name','co_applicant_race_2','co_applicant_race_3',\\\n      'co_applicant_race_4','co_applicant_race_5','co_applicant_race_name_1', 'co_applicant_race_name_2','co_applicant_race_name_3', 'co_applicant_race_name_4',\\\n       'co_applicant_race_name_5','co_applicant_sex_name','county_name',\\\n       'denial_reason_1', 'denial_reason_2', 'denial_reason_3',\\\n       'denial_reason_name_1', 'denial_reason_name_2',\\\n       'denial_reason_name_3', 'edit_status', 'edit_status_name','hoepa_status_name','lien_status_name','loan_purpose_name','loan_type_name','msamd_name',\\\n     'owner_occupancy_name','preapproval_name','property_type_name','purchaser_type_name','state_abbr','state_name','rate_spread']"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["mtg=mortgage.drop(cols,axis=1)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["## Checking missing values"],"metadata":{}},{"cell_type":"code","source":["total = mtg.isnull().sum().sort_values(ascending=False)\npercent = (mtg.isnull().sum()/mtg.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(30)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["mtg_1=mtg[(mtg['action_taken']==1)|(mtg['action_taken']==3)]"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["#Checking class imbalance\nmtg_1['action_taken'].value_counts()/len(mtg_1['action_taken'])"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["## Checking missing data by defining 2 classes"],"metadata":{}},{"cell_type":"code","source":["total = mtg_1.isnull().sum().sort_values(ascending=False)\npercent = (mtg_1.isnull().sum()/mtg.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(30)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["## Checking correlation"],"metadata":{}},{"cell_type":"code","source":["corr1=mtg_1.corr()\nf, ax = plt.subplots(figsize=(25, 10))\nsns.heatmap(corr1)\ndisplay(f)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["remcol=['county_code','number_of_owner_occupied_units','number_of_1_to_4_family_units','census_tract_number','property_type','population','owner_occupancy','hoepa_status','application_date_indicator','as_of_year','sequence_number','respondent_id','state_code','msamd','tract_to_msamd_income']"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["mtg_999=mtg_1.drop(remcol,axis=1)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["mtg_999.info()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["corr2=mtg_999.corr()\nf9, ax = plt.subplots(figsize=(25, 10))\nsns.heatmap(corr2,cmap=\"YlGnBu\")\nplt.tight_layout()\ndisplay(f9)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["## Filling missing values in Income by Linear regression"],"metadata":{}},{"cell_type":"code","source":["filtered_df = mtg_1[mtg_1['applicant_income_000s'].notnull()]"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["filtered_df['action_taken'].value_counts()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["#fig, ax = plt.subplots()\n#ax=sns.distplot(filtered_df['applicant_income_000s'])\n#display(fig)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["#Checking outliers\n#f, ax = plt.subplots(figsize=(15, 6))\n#ax=sns.boxplot(x=\"action_taken\", y=\"applicant_income_000s\", data=mtg_1)\n#display(f)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["#mtg_1[(mtg_1['applicant_income_000s']>2000) & (mtg_1['action_taken']==3)].count()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["#mtg_2=mtg_1[mtg_1['applicant_income_000s']<600]"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["#f, ax = plt.subplots(figsize=(15, 6))\n#ax=sns.boxplot(x=\"action_taken\", y=\"applicant_income_000s\", data=mtg_2)\n#display(f)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["mtg_1['msamd'].fillna(value=0, inplace=True)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["total = mtg_1.isnull().sum().sort_values(ascending=False)\npercent = (mtg_1.isnull().sum()/mtg.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(30)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["mtg_3=mtg_1\nmtg_3.head()"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["mtg_3= mtg_3[mtg_3['number_of_1_to_4_family_units'].notnull()]\nmtg_3= mtg_3[mtg_3['number_of_owner_occupied_units'].notnull()]\nmtg_3= mtg_3[mtg_3['tract_to_msamd_income'].notnull()]\nmtg_3= mtg_3[mtg_3['minority_population'].notnull()]\nmtg_3= mtg_3[mtg_3['population'].notnull()]\nmtg_3= mtg_3[mtg_3['hud_median_family_income'].notnull()]\nmtg_3= mtg_3[mtg_3['census_tract_number'].notnull()]\nmtg_3= mtg_3[mtg_3['county_code'].notnull()]"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["mtg_4= mtg_3[mtg_3['applicant_income_000s'].notnull()]"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["total = mtg_3.isnull().sum().sort_values(ascending=False)\npercent = (mtg_3.isnull().sum()/mtg.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(30)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["mtg_3.info()"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["#train test split\ntest1=mtg_3[mtg_3.applicant_income_000s.isnull()]\ntrain1= mtg_3[mtg_3.applicant_income_000s.notnull()]\ntest1.head()"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["train_1=spark.createDataFrame(train1)\ntest_1=spark.createDataFrame(test1)\n"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["display(train_1)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["a=train_1.columns\na.remove('applicant_income_000s')\na.remove('respondent_id')\na"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["train_1.printSchema()"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["from pyspark.ml.feature import StandardScaler\nva = feature.VectorAssembler(inputCols=['loan_amount_000s'], outputCol='features')\nassembled_train_df = va.transform(train_1)\nassembled_test_df = va.transform(test_1)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["training_df, validation_df = assembled_train_df.randomSplit([0.7, 0.3])"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["scaler = StandardScaler(inputCol='features', outputCol=\"scaledFeatures\",withStd=True, withMean=False)\n# Compute summary statistics by fitting the StandardScaler\nscalerModel = scaler.fit(assembled_train_df)\n# Normalize each feature to have unit standard deviation.\nscaled_train = scalerModel.transform(assembled_train_df)\nscaled_test = scalerModel.transform(assembled_test_df)"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["display(scaled_test)"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["lr = regression.LinearRegression(featuresCol='scaledFeatures', labelCol='applicant_income_000s')\npipe = Pipeline(stages=[scaler, lr])"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["pipe_model = pipe.fit(training_df)"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["pipe_model.transform(validation_df)"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["display(pipe_model.transform(validation_df))"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["rmse = fn.sqrt(fn.avg((fn.col('applicant_income_000s') - fn.col('prediction'))**2))"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["pipe_model.transform(validation_df).select(rmse).show()"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["pipe_model.transform(validation_df).select(fn.col('prediction'),fn.col('applicant_income_000s')).show()"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["hmda_new_01= pipe_model.transform(assembled_test_df)"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["display(hmda_new_01)"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["test2= hmda_new_01.toPandas()"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["test2.head()"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["test2.drop('features',axis=1,inplace=True)\ntest2.drop('scaledFeatures',axis=1,inplace=True)\ntest2.drop('applicant_income_000s',axis=1,inplace=True)"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["test2.info()"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["test2 = test2.rename(columns={'prediction' : 'applicant_income_000s'})"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["hmda_imp=pd.concat([train1, test2], ignore_index=True)"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["hmda_imp.info()"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"markdown","source":["## Removing all the missing values"],"metadata":{}},{"cell_type":"code","source":["# starting off with all values removed\n#total = mtg_4.isnull().sum().sort_values(ascending=False)\npercent = (mtg_4.isnull().sum()/mtg.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(30)"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["mtg_4['action_taken'].value_counts()"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["mtg_4['action_taken'].replace(\n    to_replace=[3],\n    value=0,\n    inplace=True\n)"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"markdown","source":["## Creating spark df"],"metadata":{}},{"cell_type":"code","source":["#hmda=spark.createDataFrame(mtg_4)"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["#display(hmda)"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":["## Keeping all the features"],"metadata":{}},{"cell_type":"code","source":["#columns\n#b=hmda.columns"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["#b.remove('respondent_id')\nb.remove('sequence_number')\nb.remove('action_taken')"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["# Train test split\n#training_df_hmda, test_df_hmda = hmda.randomSplit([0.7, 0.3],123)"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"markdown","source":["## Simple Logistic Regression without cross validation and regularization"],"metadata":{}},{"cell_type":"code","source":["#vector assembler\n#va3= feature.VectorAssembler(inputCols=b, outputCol='features')\n"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"code","source":["# Logistic regression with all features\nfrom pyspark.ml import classification\nlogis = classification.LogisticRegression(featuresCol='features', labelCol='action_taken')\n"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["#pipe_hmda = Pipeline(stages=[va3, logis])"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"code","source":["#pipe_hmda_fitted=pipe_hmda.fit(training_df_hmda)"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"code","source":["#pipe_hmda_fitted.transform(test_df_hmda).select(fn.col('probability'),fn.col('action_taken')).show(100)"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"markdown","source":["## Defining Evaluators"],"metadata":{}},{"cell_type":"code","source":["def binary_evaluation(model_pipeline, model_fitted, data):\n  return BinaryClassificationEvaluator(labelCol=model_pipeline.getStages()[-1].getLabelCol(), \n                                rawPredictionCol=model_pipeline.getStages()[-1].getRawPredictionCol()).\\\n    evaluate(model_fitted.transform(data))"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["def accuracy(predlbls):\n    counttotal = predlbls.count()\n    correct = predlbls.filter(col('action_taken') == col(\"prediction\")).count()\n    wrong = predlbls.filter(col('action_taken') != col(\"prediction\")).count()\n    ratioCorrect = float(correct)/counttotal\n    print(\"Correct: {0}, Wrong: {1}, Model Accuracy: {2}\".format(correct, wrong, np.round(ratioCorrect, 2)))"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"code","source":["#binary_evaluation(pipe_hmda,pipe_hmda_fitted,test_df_hmda)"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"code","source":["#accuracy(pipe_hmda_fitted.transform(test_df_hmda))"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"code","source":["#training_df_hmda.groupby('action_taken').count().show()"],"metadata":{},"outputs":[],"execution_count":94},{"cell_type":"code","source":["#scaled_feat = StandardScaler(inputCol='features', outputCol=\"scaledFeatures\",withStd=True, withMean=False)\n# Compute summary statistics by fitting the StandardScaler\n#scalerModel = scaler.fit(scaled_hmda)"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"code","source":["#from pyspark.ml import classification\n#logis = classification.LogisticRegression(featuresCol='scaledFeatures', labelCol='action_taken')\n#pipe_logis = Pipeline(stages=[scaled_feat,logis])"],"metadata":{},"outputs":[],"execution_count":96},{"cell_type":"markdown","source":["## Removing un-related columns"],"metadata":{}},{"cell_type":"code","source":["cols=hmda.columns\ncols.remove('county_code')\ncols.remove('number_of_owner_occupied_units')\ncols.remove('number_of_1_to_4_family_units')\ncols.remove('census_tract_number')\ncols.remove('property_type')\ncols.remove('population')\ncols.remove('owner_occupancy')\ncols.remove('hoepa_status')\ncols.remove('application_date_indicator')\ncols.remove('as_of_year')\ncols.remove('sequence_number')\ncols.remove('respondent_id')\ncols.remove('action_taken')"],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"code","source":["cols"],"metadata":{},"outputs":[],"execution_count":99},{"cell_type":"markdown","source":["## Using some features"],"metadata":{}},{"cell_type":"code","source":["#model1 = Pipeline(stages=[feature.VectorAssembler(inputCols=['purchaser_type'],\n                                        outputCol='features'),\n                 classification.LogisticRegression(labelCol='action_taken', featuresCol='features',regParam=0,elasticNetParam=0)])"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"code","source":["#model1_fitted = model1.fit(training_df_hmda)"],"metadata":{},"outputs":[],"execution_count":102},{"cell_type":"code","source":["#accuracy(model1_fitted.transform(test_df_hmda))"],"metadata":{},"outputs":[],"execution_count":103},{"cell_type":"code","source":["#binary_evaluation(model1,model1_fitted,test_df_hmda)"],"metadata":{},"outputs":[],"execution_count":104},{"cell_type":"markdown","source":["## Fitting features present in cols"],"metadata":{}},{"cell_type":"code","source":["#model2 = Pipeline(stages=[feature.VectorAssembler(inputCols=cols,\n                                        #outputCol='features'),\n                 #logis])"],"metadata":{},"outputs":[],"execution_count":106},{"cell_type":"code","source":["#model2_fitted = model2.fit(training_df_hmda)"],"metadata":{},"outputs":[],"execution_count":107},{"cell_type":"code","source":["#binary_evaluation(model2,model2_fitted,test_df_hmda)"],"metadata":{},"outputs":[],"execution_count":108},{"cell_type":"code","source":["#accuracy(model2_fitted.transform(test_df_hmda))"],"metadata":{},"outputs":[],"execution_count":109},{"cell_type":"code","source":["#paramGrid = ParamGridBuilder() \\\n    .addGrid(logis.elasticNetParam, [0.0, 0.1, 0.01]) \\\n    .addGrid(logis.regParam, [0.0,0.1, 0.01]) \\\n    .build()"],"metadata":{},"outputs":[],"execution_count":110},{"cell_type":"code","source":["#Cross validation\n#crossval_01 = CrossValidator(estimator=model2, \n                          #estimatorParamMaps=paramGrid, \n                          #evaluator= evaluator, \n                          #numFolds=2)"],"metadata":{},"outputs":[],"execution_count":111},{"cell_type":"code","source":["#crossval_fitted_01 = crossval_01.fit(training_df_hmda)"],"metadata":{},"outputs":[],"execution_count":112},{"cell_type":"code","source":["#accuracy(crossval_fitted_01.transform(test_df_hmda))"],"metadata":{},"outputs":[],"execution_count":113},{"cell_type":"code","source":["#evaluator.evaluate(crossval_fitted_01.transform(test_df_hmda))"],"metadata":{},"outputs":[],"execution_count":114},{"cell_type":"markdown","source":["## Random Forest Classifier"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier"],"metadata":{},"outputs":[],"execution_count":116},{"cell_type":"code","source":["rf=classification.RandomForestClassifier(labelCol='action_taken', featuresCol='features',maxDepth=8,numTrees=200)\npipe_rf= Pipeline(stages=[feature.VectorAssembler(inputCols=cols,\n                                        outputCol='features'),rf])"],"metadata":{},"outputs":[],"execution_count":117},{"cell_type":"code","source":["pipe_rf_fitted=pipe_rf.fit(training_df_hmda)"],"metadata":{},"outputs":[],"execution_count":118},{"cell_type":"code","source":["binary_evaluation(pipe_rf,pipe_rf_fitted,test_df_hmda)"],"metadata":{},"outputs":[],"execution_count":119},{"cell_type":"code","source":["accuracy(pipe_rf_fitted.transform(test_df_hmda))"],"metadata":{},"outputs":[],"execution_count":120},{"cell_type":"code","source":["#evaluator_rf = BinaryClassificationEvaluator(labelCol=\"action_taken\")"],"metadata":{},"outputs":[],"execution_count":121},{"cell_type":"markdown","source":["## Using with more trees"],"metadata":{}},{"cell_type":"code","source":["#rf_01=classification.RandomForestClassifier(labelCol='action_taken', featuresCol='features',maxDepth=8,numTrees=400)\n#pipe_rf_01= Pipeline(stages=[feature.VectorAssembler(inputCols=cols,\n                                        outputCol='features'),rf_01])"],"metadata":{},"outputs":[],"execution_count":123},{"cell_type":"code","source":["#pipe_rf_fitted_01=pipe_rf_01.fit(training_df_hmda)"],"metadata":{},"outputs":[],"execution_count":124},{"cell_type":"code","source":["#binary_evaluation(pipe_rf_01,pipe_rf_fitted_01,test_df_hmda)"],"metadata":{},"outputs":[],"execution_count":125},{"cell_type":"code","source":["#accuracy(pipe_rf_fitted_01.transform(test_df_hmda))"],"metadata":{},"outputs":[],"execution_count":126},{"cell_type":"markdown","source":["## Using data from dataframe that has imputed values for Income"],"metadata":{}},{"cell_type":"code","source":["hmda_imp['action_taken'].replace(\n    to_replace=[3],\n    value=0,\n    inplace=True\n)"],"metadata":{},"outputs":[],"execution_count":128},{"cell_type":"code","source":["hmda_imp['action_taken'].value_counts()"],"metadata":{},"outputs":[],"execution_count":129},{"cell_type":"code","source":["#creating spark df\nhmda_2=spark.createDataFrame(hmda_imp)"],"metadata":{},"outputs":[],"execution_count":130},{"cell_type":"code","source":["display(hmda_2)"],"metadata":{},"outputs":[],"execution_count":131},{"cell_type":"code","source":["cols1=hmda_2.columns\ncols1.remove('county_code')\ncols1.remove('number_of_owner_occupied_units')\ncols1.remove('number_of_1_to_4_family_units')\ncols1.remove('census_tract_number')\ncols1.remove('property_type')\ncols1.remove('population')\ncols1.remove('owner_occupancy')\ncols1.remove('hoepa_status')\ncols1.remove('application_date_indicator')\ncols1.remove('as_of_year')\ncols1.remove('sequence_number')\ncols1.remove('respondent_id')\ncols1.remove('action_taken')\n"],"metadata":{},"outputs":[],"execution_count":132},{"cell_type":"code","source":["training_df_hmda_1, test_df_hmda_1 = hmda_2.randomSplit([0.7, 0.3],123)"],"metadata":{},"outputs":[],"execution_count":133},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":134},{"cell_type":"code","source":["va001= feature.VectorAssembler(inputCols=cols1,\n                                        outputCol='features')"],"metadata":{},"outputs":[],"execution_count":135},{"cell_type":"code","source":["assembled_train=va001.transform(training_df_hmda_1)\nassembled_test= va001.transform(test_df_hmda_1)"],"metadata":{},"outputs":[],"execution_count":136},{"cell_type":"code","source":["log_reg = classification.LogisticRegression(featuresCol='features', labelCol='action_taken')\n\npipeline = Pipeline(stages=[feature.VectorAssembler(inputCols=cols1,\n                                        outputCol='features'),\n                logis])\n"],"metadata":{},"outputs":[],"execution_count":137},{"cell_type":"code","source":["paramGrid = ParamGridBuilder() \\\n    .addGrid(logis.elasticNetParam, [0.0, 0.1, 0.01]) \\\n    .addGrid(logis.regParam, [0.0,0.1, 0.01,.001 ]) \\\n    .build()"],"metadata":{},"outputs":[],"execution_count":138},{"cell_type":"code","source":["evaluator = BinaryClassificationEvaluator(labelCol='action_taken')"],"metadata":{},"outputs":[],"execution_count":139},{"cell_type":"code","source":["#Cross validation\ncrossVal = CrossValidator(estimator=pipeline, \n                          estimatorParamMaps=paramGrid, \n                          evaluator= evaluator, \n                          numFolds=2)"],"metadata":{},"outputs":[],"execution_count":140},{"cell_type":"code","source":["cvModel = crossVal.fit(training_df_hmda_1)"],"metadata":{},"outputs":[],"execution_count":141},{"cell_type":"code","source":["cvModel.bestModel.stages"],"metadata":{},"outputs":[],"execution_count":142},{"cell_type":"code","source":["display(cvModel.transform(test22))"],"metadata":{},"outputs":[],"execution_count":143},{"cell_type":"code","source":["cvModel.bestModel.stages[1]\n"],"metadata":{},"outputs":[],"execution_count":144},{"cell_type":"code","source":["best_log_reg_model = cvModel.bestModel.stages[1]\nbest_log_reg_model"],"metadata":{},"outputs":[],"execution_count":145},{"cell_type":"code","source":["best_log_reg_model.extractParamMap()"],"metadata":{},"outputs":[],"execution_count":146},{"cell_type":"code","source":["for param in ['regParam', 'elasticNetParam', 'maxIter', 'tol']:\n    best_log_reg_model.explainParam(param)"],"metadata":{},"outputs":[],"execution_count":147},{"cell_type":"code","source":["grid_score_param_records = []\nfor metric, param_map in zip(cvModel.avgMetrics, paramGrid):\n    grid_score_param_records.append(list(param_map.values()) + [metric])"],"metadata":{},"outputs":[],"execution_count":148},{"cell_type":"code","source":["grid_score_param_df = pd.DataFrame(grid_score_param_records, columns=['regParam', 'elasticNetParam', 'areaUnderROC'])"],"metadata":{},"outputs":[],"execution_count":149},{"cell_type":"code","source":["grid_score_param_df"],"metadata":{},"outputs":[],"execution_count":150},{"cell_type":"code","source":["grid_score_param_df = pd.pivot_table(grid_score_param_df, index=['regParam', 'elasticNetParam'], values=['areaUnderROC'])"],"metadata":{},"outputs":[],"execution_count":151},{"cell_type":"code","source":["grid_score_param_df"],"metadata":{},"outputs":[],"execution_count":152},{"cell_type":"code","source":["grid_score_param_df['areaUnderROC'].unstack()"],"metadata":{},"outputs":[],"execution_count":153},{"cell_type":"code","source":["# Compute the heat map\nhmap = grid_score_param_df['areaUnderROC'].unstack()\nf2, ax = plt.subplots(figsize=(6, 4))\n# Set up the matplotlib figure\nsns.heatmap(hmap, square=False, annot=True, cmap='viridis', fmt='.4g', linewidths=1)\nplt.title('Heat Map of Grid Search Parameters')\nplt.tight_layout()\ndisplay(f2)"],"metadata":{},"outputs":[],"execution_count":154},{"cell_type":"code","source":["model = log_reg.fit(assembled_train)"],"metadata":{},"outputs":[],"execution_count":155},{"cell_type":"code","source":["test_summary = model.evaluate(assembled_test)"],"metadata":{},"outputs":[],"execution_count":156},{"cell_type":"code","source":["test_summary.accuracy"],"metadata":{},"outputs":[],"execution_count":157},{"cell_type":"code","source":["test_summary()"],"metadata":{},"outputs":[],"execution_count":158},{"cell_type":"code","source":["test_summary.roc.limit(10).toPandas()"],"metadata":{},"outputs":[],"execution_count":159},{"cell_type":"code","source":["test_roc_pdf = test_summary.roc.toPandas()"],"metadata":{},"outputs":[],"execution_count":160},{"cell_type":"code","source":["plt.figure(figsize=(6,4))\nplt.plot(test_roc_pdf['FPR'], test_roc_pdf['TPR'], lw=1, label='logistic Classifier AUC = %0.2f' % (test_summary.areaUnderROC))\nplt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='NULL Accuracy')\nplt.title('ROC AUC Curve')\nplt.tight_layout()\nplt.legend(loc=\"best\" )\ndisplay()"],"metadata":{},"outputs":[],"execution_count":161},{"cell_type":"code","source":["test_preds = cvModel.transform(test_df_hmda_1)"],"metadata":{},"outputs":[],"execution_count":162},{"cell_type":"code","source":["cv_test_areaUnderROC = evaluator.evaluate(test_preds)\ncv_test_areaUnderROC"],"metadata":{},"outputs":[],"execution_count":163},{"cell_type":"code","source":["testpredlbls = test_preds.select(\"prediction\", \"action_taken\")"],"metadata":{},"outputs":[],"execution_count":164},{"cell_type":"code","source":["cvModel.bestModel.stages[1]"],"metadata":{},"outputs":[],"execution_count":165},{"cell_type":"code","source":["print(\"Coeff :\" + str(cvModel.bestModel.stages[1].coefficientMatrix))\nprint(\"intercepts :\" + str(cvModel.bestModel.stages[1].interceptVector))"],"metadata":{},"outputs":[],"execution_count":166},{"cell_type":"code","source":["coeff=(cvModel.bestModel.stages[1].coefficientMatrix)\ncoeff=coeff.toArray()\ncoeff1=np.array(coeff).tolist()\ncoeff1"],"metadata":{},"outputs":[],"execution_count":167},{"cell_type":"code","source":["coeff1=[-0.114871852712418,\n  0.0009155153068500655,\n  0.0008821364731822005,\n  0.038890706161297174,\n  0.014961989368882204,\n  -0.1826686237195676,\n  0.0016804793278324965,\n  0.06817042688115463,\n  2.6589042009618525e-07,\n  -0.5250199678929605,\n  -0.000279462784692149,\n  -0.49518827654134034,\n  -0.08355833183166382,\n  -0.0066203628512083,\n  7.1984832166973485e-06,\n  -0.1348451600724103,\n  0.0,\n  0.0019111227562408969]"],"metadata":{},"outputs":[],"execution_count":168},{"cell_type":"code","source":["d={'Features': cols1 , 'Coeff': coeff1}\ncoeffdf= pd.DataFrame(data=d)\ncoeffdf"],"metadata":{},"outputs":[],"execution_count":169},{"cell_type":"code","source":["f8, ax = plt.subplots(figsize=(15, 6))\nax = sns.barplot(x=\"Features\", y=\"Coeff\", data=coeffdf)\nax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\nplt.title(\"Logistic regression coefficients\")\nplt.tight_layout()\ndisplay(f8)"],"metadata":{},"outputs":[],"execution_count":170},{"cell_type":"markdown","source":["## Random Forest"],"metadata":{}},{"cell_type":"code","source":["rf=classification.RandomForestClassifier(labelCol='action_taken', featuresCol='features',maxDepth=8,numTrees=200)\npipe_rf= Pipeline(stages=[feature.VectorAssembler(inputCols=cols1,\n                                        outputCol='features'),rf])"],"metadata":{},"outputs":[],"execution_count":172},{"cell_type":"code","source":["pipe_rf_fitted=pipe_rf.fit(training_df_hmda_1)"],"metadata":{},"outputs":[],"execution_count":173},{"cell_type":"code","source":["binary_evaluation(pipe_rf,pipe_rf_fitted,test_df_hmda_1)"],"metadata":{},"outputs":[],"execution_count":174},{"cell_type":"code","source":["accuracy(pipe_rf_fitted.transform(test_df_hmda_1))"],"metadata":{},"outputs":[],"execution_count":175},{"cell_type":"code","source":["display(pipe_rf_fitted.transform(test_df_hmda_1))"],"metadata":{},"outputs":[],"execution_count":176},{"cell_type":"code","source":["display(training_df_hmda_1)"],"metadata":{},"outputs":[],"execution_count":177},{"cell_type":"code","source":["d = {'agency_code': 3, 'applicant_ethnicity2': 2,'applicant_race_1': 3, 'applicant_sex': 2 ,'co_applicant_ethnicity':2,'co_applicant_race_1':4 ,'co_applicant_sex':1 \\\n   ,'hud_median_family_income':70,'lien_status':4, 'loan_amount_000s':10,'loan_purpose':1,'loan_type':3,'minority_population':5,'msamd':15380, 'preapproval':3,'state_code':36,'tract_to_msamd_income':80 }"],"metadata":{},"outputs":[],"execution_count":178},{"cell_type":"code","source":["d2= {'respondent_id': u'0000451965', 'loan_purpose': 1, 'minority_population': 18878.770000457763672, 'tract_to_msamd_income': 138.44000244140625, 'as_of_year': 2015, 'co_applicant_ethnicity': 2, 'county_code': 64.0, 'property_type': 1,  'purchaser_type': 0, 'agency_code': 4, 'hoepa_status': 2, 'applicant_ethnicity': 2, 'number_of_1_to_4_family_units': 249.0, 'msamd': 30824.0, 'co_applicant_sex': 2, 'loan_amount_000s': 10, 'preapproval': 1, 'co_applicant_race_1': 5, 'loan_type': 3, 'state_code': 36, 'number_of_owner_occupied_units': 2077.0, 'population': 7442.0, 'owner_occupancy': 1,'purchaser_type':2, 'application_date_indicator': 0, 'census_tract_number': 319.0, 'lien_status': 2, 'applicant_sex': 1, 'hud_median_family_income': 100.0, 'applicant_income_000s': 760.0, 'applicant_race_1': 5, 'sequence_number': 155}"],"metadata":{},"outputs":[],"execution_count":179},{"cell_type":"code","source":["test=pd.DataFrame(d2, index=[0])"],"metadata":{},"outputs":[],"execution_count":180},{"cell_type":"code","source":["test22=spark.createDataFrame(test)"],"metadata":{},"outputs":[],"execution_count":181},{"cell_type":"markdown","source":["## Trying to predict"],"metadata":{}},{"cell_type":"code","source":["pred22=pipe_rf_fitted.transform(test22)"],"metadata":{},"outputs":[],"execution_count":183},{"cell_type":"code","source":["display(pred22)"],"metadata":{},"outputs":[],"execution_count":184},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":185},{"cell_type":"code","source":["#hmda_imp1=hmda_imp.iloc[7]"],"metadata":{},"outputs":[],"execution_count":186},{"cell_type":"code","source":["#print(hmda_imp1.to_dict())"],"metadata":{},"outputs":[],"execution_count":187},{"cell_type":"code","source":["#from pyspark.mllib.evaluation import BinaryClassificationMetrics as metric\nresults = pipe_rf_fitted.transform(test_df_hmda_1).select(['probability', 'action_taken'])\n \n## prepare score-label set\nresults_collect = results.collect()\nresults_list = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect]\nscoreAndLabels = sc.parallelize(results_list)\n \nmetrics = metric(scoreAndLabels)\nprint(\"The ROC score is (@numTrees=200): \", metrics.areaUnderROC)\n"],"metadata":{},"outputs":[],"execution_count":188},{"cell_type":"markdown","source":["## Gradient Boosting"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import GBTClassifier\ngbt = GBTClassifier(labelCol=\"action_taken\", featuresCol=\"features\", maxIter=10)\npipe_gbt= Pipeline(stages=[feature.VectorAssembler(inputCols=cols1,\n                                        outputCol='features'),gbt])"],"metadata":{},"outputs":[],"execution_count":190},{"cell_type":"code","source":["pipe_gbt_fitted=pipe_gbt.fit(training_df_hmda_1)"],"metadata":{},"outputs":[],"execution_count":191},{"cell_type":"code","source":["evaluator.evaluate(pipe_gbt_fitted.transform(test_df_hmda_1))"],"metadata":{},"outputs":[],"execution_count":192},{"cell_type":"code","source":["accuracy(pipe_gbt_fitted.transform(test_df_hmda_1))"],"metadata":{},"outputs":[],"execution_count":193},{"cell_type":"markdown","source":["## Try to predict"],"metadata":{}},{"cell_type":"code","source":["display(pipe_gbt_fitted.transform(test22))"],"metadata":{},"outputs":[],"execution_count":195},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":196},{"cell_type":"code","source":["display(trans)"],"metadata":{},"outputs":[],"execution_count":197},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":198}],"metadata":{"name":"718 Final Project","notebookId":1299751309215257},"nbformat":4,"nbformat_minor":0}
